{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation with Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.utils import simple_preprocess\n",
    "import glob\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import statistics\n",
    "import svmlight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pang et al. dataset (2000 reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_from_file(file_path):\n",
    "    # Get list of unigrams from a file\n",
    "    with open(file_path, 'r') as f:\n",
    "        features = simple_preprocess(f.read())\n",
    "    return features\n",
    "\n",
    "def get_all_features():\n",
    "    # Get all truples of (sentiment, file_name, list_of_unigrams)\n",
    "    pos_path = \"../POS\"\n",
    "    neg_path = \"../NEG\"\n",
    "\n",
    "    pos_files = sorted(list(filter(lambda x: x.endswith(\".tag\"), os.listdir(pos_path))))\n",
    "    neg_files = sorted(list(filter(lambda x: x.endswith(\".tag\"), os.listdir(neg_path))))\n",
    "\n",
    "    features = []\n",
    "\n",
    "    for f in pos_files:\n",
    "        features.append((\"POS\", f, get_features_from_file(pos_path + \"/\" + f)))\n",
    "\n",
    "    for f in neg_files:\n",
    "        features.append((\"NEG\", f, get_features_from_file(neg_path + \"/\" + f)))\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index each word in a set starting from 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_indices(data_set):\n",
    "    # Create a dictionary assigning each word an index starting from 1\n",
    "    feature_indices = {}\n",
    "    index = 1\n",
    "    for (sentiment, file_name, features) in data_set:\n",
    "        for w in features:\n",
    "            if w not in feature_indices:\n",
    "                feature_indices[w] = index\n",
    "                index += 1\n",
    "    return feature_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM using unigrams + frequency/presence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm(training_set, test_set, presence):\n",
    "    feature_indices = get_feature_indices(training_set + test_set)\n",
    "\n",
    "    # Train the SVM model on the training set\n",
    "    formatted_training_set = []\n",
    "\n",
    "    for (sentiment, file_name, features) in training_set:\n",
    "        feature_vec = []\n",
    "        feature_freqs = {}\n",
    "\n",
    "        for w in features:\n",
    "            if presence or w not in feature_freqs:\n",
    "                feature_freqs[w] = 1\n",
    "            else:\n",
    "                feature_freqs[w] += 1\n",
    "\n",
    "        for word, count in feature_freqs.items():\n",
    "            feature_vec.append((feature_indices[word], count))\n",
    "\n",
    "        list.sort(feature_vec, key=lambda x: x[0])\n",
    "\n",
    "        sent_val = 1 if sentiment == \"POS\" else -1\n",
    "        formatted_training_set.append((sent_val, feature_vec))\n",
    "\n",
    "    model = svmlight.learn(formatted_training_set)\n",
    "    \n",
    "    # Test the SVM model on the test set\n",
    "    formatted_test_set = []\n",
    "    \n",
    "    for (sentiment, file_name, features) in test_set:\n",
    "        feature_vec = []\n",
    "        feature_freqs = {}\n",
    "\n",
    "        for w in features:\n",
    "            if presence or w not in feature_freqs:\n",
    "                feature_freqs[w] = 1\n",
    "            else:\n",
    "                feature_freqs[w] += 1\n",
    "\n",
    "        for word, count in feature_freqs.items():\n",
    "            feature_vec.append((feature_indices[word], count))\n",
    "\n",
    "        list.sort(feature_vec, key=lambda x: x[0])\n",
    "        formatted_test_set.append((0, feature_vec))\n",
    "\n",
    "    predictions = svmlight.classify(model, formatted_test_set)\n",
    "\n",
    "    # Format the predictions into truples of (predicted_sentiment, actual_sentiment, file_name)\n",
    "    formatted_predictions = []\n",
    "    idx = 0\n",
    "    for (sentiment, file_name, features) in test_set:\n",
    "        formatted_predictions.append((\"POS\" if predictions[idx] > 0 else \"NEG\", sentiment, file_name))\n",
    "        idx += 1\n",
    "    \n",
    "    return formatted_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_train = '../aclImdb/train/neg/*'\n",
    "pos_train = '../aclImdb/train/pos/*'\n",
    "unsup_train = '../aclImdb/train/unsup/*'\n",
    "neg_test = '../aclImdb/test/neg/*'\n",
    "pos_test = '../aclImdb/test/pos/*'\n",
    "train_corpus = None\n",
    "\n",
    "\n",
    "def read_docs(paths_list):\n",
    "    # Return a list of TaggedDocuments from a list of paths\n",
    "    for i in range(len(paths_list)):\n",
    "        with open(paths_list[i]) as doc:\n",
    "            tokens = simple_preprocess(doc.readline())\n",
    "            \n",
    "            # We tag each document\n",
    "            yield TaggedDocument(tokens, [i])\n",
    "\n",
    "\n",
    "train_path_list = glob.glob(neg_train)\n",
    "train_path_list.extend(glob.glob(pos_train))\n",
    "train_path_list.extend(glob.glob(unsup_train))\n",
    "train_path_list.extend(glob.glob(neg_test))\n",
    "train_path_list.extend(glob.glob(pos_test))\n",
    "\n",
    "train_corpus = list(read_docs(train_path_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Doc2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_doc2vec_model(model_file, params):\n",
    "    print \"Creating Doc2Vec model with:\"\n",
    "    print \"Distributed memory: \" + str(params['dm'])\n",
    "    print \"Vector size: \" + str(params['vector_size'])\n",
    "    print \"Min count: \" + str(params['min_count'])\n",
    "    print \"Epochs: \" + str(params['epochs'])\n",
    "    print \"Workers: \" + str(4)\n",
    "    print \"Hierarchical softmax: \" + str(params['hs'])\n",
    "    print \"Window size: \" + str(params['window'])\n",
    "    print \"Negative sampling: \" + str(params['negative'])\n",
    "    print \"...\"\n",
    "\n",
    "    model = Doc2Vec(seed=0, dbow_words=1, dm=params['dm'], vector_size=params['vector_size'],\n",
    "                   min_count=params['min_count'], epochs=params['epochs'], workers=4,\n",
    "                   hs=params['hs'], window=params['window'])\n",
    "    model.build_vocab(train_corpus)\n",
    "    model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "    model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "\n",
    "    model.save(model_file)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM using Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_with_doc2vec(doc2vec_model_file, training_set, test_set):\n",
    "    doc2vec_model = Doc2Vec.load(doc2vec_model_file)\n",
    "        \n",
    "    # Train the SVM model on the training set using Doc2Vec embeddings\n",
    "    formatted_training_set = []\n",
    "\n",
    "    for (sentiment, file_name, features) in training_set:\n",
    "        vec = [(i+1, p) for i, p in enumerate(doc2vec_model.infer_vector(features))]\n",
    "        sent_val = 1 if sentiment == \"POS\" else -1\n",
    "        formatted_training_set.append((sent_val, vec))\n",
    "      \n",
    "    svm_model = svmlight.learn(formatted_training_set, type='classification')\n",
    "   \n",
    "    # Test the SVM model on the test set\n",
    "    formatted_test_set = []\n",
    "    \n",
    "    for (sentiment, file_name, features) in test_set:\n",
    "        vec = [(i+1, p) for i, p in enumerate(doc2vec_model.infer_vector(features))]\n",
    "        formatted_test_set.append((0, vec))\n",
    "\n",
    "    predictions = svmlight.classify(svm_model, formatted_test_set)\n",
    "\n",
    "    # Format the predictions into truples of (predicted_sentiment, actual_sentiment, file_name)\n",
    "    formatted_predictions = []\n",
    "    idx = 0\n",
    "    for (sentiment, file_name, features) in test_set:\n",
    "        formatted_predictions.append((\"POS\" if predictions[idx] > 0 else \"NEG\", sentiment, file_name))\n",
    "        idx += 1\n",
    "\n",
    "    return formatted_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Round-Robin splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_count = 10\n",
    "\n",
    "def round_robin_split(features):\n",
    "    splits = []\n",
    "    for i in range(0, fold_count):\n",
    "        splits.append([])\n",
    "\n",
    "    pos_features = features[0:(len(features)/2)]\n",
    "    neg_features = features[(len(features)/2):len(features)]\n",
    "\n",
    "    for i in range(0, (len(features)/2)):\n",
    "        splits[i % fold_count].append(pos_features[i])\n",
    "        splits[i % fold_count].append(neg_features[i])\n",
    "\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train SVM model using the first split as the validation set and a given Doc2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_training(model_file):\n",
    "    splits = round_robin_split(get_all_features())\n",
    "    \n",
    "    training_set = []\n",
    "    validation_set = []\n",
    "    \n",
    "    for i in range(len(splits)):\n",
    "        if i == 0:\n",
    "            validation_set = splits[i]\n",
    "        else:\n",
    "            training_set.extend(splits[i])\n",
    "            \n",
    "    predictions = svm_with_doc2vec(model_file, training_set, validation_set)\n",
    "    \n",
    "    right_predictions = 0\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i][0] == predictions[i][1]:\n",
    "            right_predictions += 1\n",
    "    \n",
    "    # print [1 if predictions[i][0] == predictions[i][1] else 0 for i in range(len(predictions))]\n",
    "    \n",
    "    return float(right_predictions) / float(len(predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search():\n",
    "    dms = [0]\n",
    "    vector_sizes = range(110,151)\n",
    "    min_counts = range(0,16)\n",
    "    epochss = range(10,21)\n",
    "    hss = [0, 1]\n",
    "    windows = range(10,21)\n",
    "    negatives = range(5,21)\n",
    "    i = 65\n",
    "    with open(\"better.txt\", \"w+\") as fp:\n",
    "        for _ in range(20):\n",
    "            model_file = './models/model' + str(i) + '.modelFile'\n",
    "            params = {\n",
    "                'dm': dms[0],\n",
    "                'vector_size': vector_sizes[random.randint(0,len(vector_sizes)-1)],\n",
    "                'min_count': min_counts[random.randint(0,len(min_counts)-1)],\n",
    "                'epochs': epochss[random.randint(0,len(epochss)-1)],\n",
    "                'hs': random.randint(0,1),\n",
    "                'window': windows[random.randint(0,len(windows)-1)],\n",
    "                'negative': negatives[random.randint(0,len(negatives)-1)]\n",
    "            }\n",
    "            create_doc2vec_model(model_file, params)\n",
    "            accuracy = svm_training(model_file)\n",
    "            print \"Model \" + str(i) + \": \" + str(accuracy) + \"\\n\\n\"\n",
    "            fp.write(\"Model \" + str(i) + \":\\n\" + str(params) + \"\\nAccuracy: \" + str(accuracy) + \"\\n\\n\")\n",
    "            i += 1\n",
    "            \n",
    "#     with open(\"resultss.txt\", \"w+\") as fp:\n",
    "#         for dm in dms:\n",
    "#             for vector_size in vector_sizes:\n",
    "#                 for min_count in min_counts:\n",
    "#                     for epochs in epochss:\n",
    "#                         for hs in hss:\n",
    "#                             for window in windows:\n",
    "#                                 for negative in negatives:\n",
    "#                                     model_file = './models/model' + str(i) + '.modelFile'\n",
    "#                                     params = {\n",
    "#                                         'dm': dm,\n",
    "#                                         'vector_size': vector_size,\n",
    "#                                         'min_count': min_count,\n",
    "#                                         'epochs': epochs,\n",
    "#                                         'hs': hs,\n",
    "#                                         'window': window,\n",
    "#                                         'negative': negative\n",
    "#                                     }\n",
    "#                                     create_doc2vec_model(model_file, params)\n",
    "#                                     accuracy = svm_training(model_file)\n",
    "#                                     print \"Model \" + str(i) + \": \" + str(accuracy) + \"\\n\\n\"\n",
    "#                                     fp.write(\"Model \" + str(i) + \":\\n\" + str(params) + \"\\nAccuracy: \" + str(accuracy) + \"\\n\\n\")\n",
    "#                                     i -= 1\n",
    "\n",
    "# grid_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove validation set from the Pang et al. dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_validation_set(data_set):\n",
    "    splits = round_robin_split(data_set)\n",
    "    \n",
    "    # Remove the first split (the validation one) as we're not using it anymore\n",
    "    remaining_pos = []\n",
    "    remaining_neg = []\n",
    "    for i in range(1,len(splits)):\n",
    "        for (sentiment, file_name, features) in splits[i]:\n",
    "            if sentiment == 'POS':\n",
    "                remaining_pos.append((sentiment, file_name, features))\n",
    "            else:\n",
    "                remaining_neg.append((sentiment, file_name, features))\n",
    "    return remaining_pos + remaining_neg\n",
    "\n",
    "remaining_data_set = remove_validation_set(get_all_features())\n",
    "smaller_splits = round_robin_split(remaining_data_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-fold cross-validation on the 1800 Pang et al. reviews using SVM with unigrams + frequencies/presence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_for_index_unigrams(splits, index, presence):\n",
    "    training_set = []\n",
    "    test_set = []\n",
    "\n",
    "    for i in range(0, len(splits)):\n",
    "        if i == index:\n",
    "            test_set = splits[i]\n",
    "        else:\n",
    "            training_set.extend(splits[i])\n",
    "\n",
    "    return svm(training_set, test_set, presence)\n",
    "\n",
    "def aggregate_predictions_unigrams(splits, presence):\n",
    "    results = []\n",
    "    for i in range(len(splits)):\n",
    "        results.extend(predictions_for_index_unigrams(splits, i, presence))\n",
    "    return results\n",
    "\n",
    "def predictions_for_svm_unigrams(presence=False):\n",
    "    return aggregate_predictions_unigrams(smaller_splits, presence)\n",
    "\n",
    "def cross_validation_for_index(splits, index, presence):\n",
    "    predictions = predictions_for_index_unigrams(splits, index, presence)\n",
    "\n",
    "    right_predictions = 0\n",
    "    for i in range(0, len(predictions)):\n",
    "        if predictions[i][0] == predictions[i][1]:\n",
    "            right_predictions += 1\n",
    "\n",
    "    return float(right_predictions) / float(len(predictions))\n",
    "\n",
    "def cross_validation(splits, presence):\n",
    "    accuracies = []\n",
    "    for i in range(0, len(splits)):\n",
    "        accuracies.append(cross_validation_for_index(splits, i, presence))\n",
    "    return accuracies\n",
    "\n",
    "def svm_with_unigrams_cross_validate(presence=False):\n",
    "    accuracies = cross_validation(smaller_splits, presence)\n",
    "    if presence:\n",
    "        print \"SVM using unigrams & presence mean accuracy: \" + str(statistics.mean(accuracies))\n",
    "    else:\n",
    "        print \"SVM using unigrams & frequencies mean accuracy: \" + str(statistics.mean(accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-fold cross-validation on the 1800 Pang et al. reviews using the best SVM + Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_for_index_doc2vec(splits, index):\n",
    "    training_set = []\n",
    "    test_set = []\n",
    "\n",
    "    for i in range(0, len(splits)):\n",
    "        if i == index:\n",
    "            test_set = splits[i]\n",
    "        else:\n",
    "            training_set.extend(splits[i])\n",
    "\n",
    "    return svm_with_doc2vec('./models/model73.modelFile', training_set, test_set)\n",
    "\n",
    "def aggregate_predictions_doc2vec(splits):\n",
    "    results = []\n",
    "    for i in range(len(splits)):\n",
    "        results.extend(predictions_for_index_doc2vec(splits, i))\n",
    "    return results\n",
    "\n",
    "def predictions_for_svm_doc2vec():\n",
    "    return aggregate_predictions_doc2vec(smaller_splits)\n",
    "\n",
    "def cross_validation_for_index_doc2vec(splits, index):\n",
    "    predictions = predictions_for_index_doc2vec(splits, index)\n",
    "\n",
    "    right_predictions = 0\n",
    "    for i in range(0, len(predictions)):\n",
    "        if predictions[i][0] == predictions[i][1]:\n",
    "            right_predictions += 1\n",
    "\n",
    "    return float(right_predictions) / float(len(predictions))\n",
    "\n",
    "def cross_validation_doc2vec(splits):\n",
    "    accuracies = []\n",
    "    for i in range(0, len(splits)):\n",
    "        accuracies.append(cross_validation_for_index_doc2vec(splits, i))\n",
    "    return accuracies\n",
    "\n",
    "def svm_with_doc2vec_cross_validate():\n",
    "    accuracies = cross_validation_doc2vec(smaller_splits)\n",
    "    print \"SVM using Doc2Vec mean accuracy: \" + str(statistics.mean(accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(predictions):\n",
    "    # A prediction = a truple of the form (predicted_sentiment, actual_sentiment, file_name)\n",
    "    right_predictions = 0.0\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i][0] == predictions[i][1]:\n",
    "            right_predictions += 1.0\n",
    "    return right_predictions/len(predictions)\n",
    "\n",
    "def perm_test(predictionsA, predictionsB, R):\n",
    "    # Sort by file name, so the files match\n",
    "    list.sort(predictionsA, key=lambda x: x[2])\n",
    "    list.sort(predictionsB, key=lambda x: x[2])\n",
    "    \n",
    "    s = 0.0\n",
    "    \n",
    "    # Calculate original mean difference\n",
    "    mean_difference = abs(compute_accuracy(predictionsA) - compute_accuracy(predictionsB))\n",
    "    \n",
    "    for _ in range(R):\n",
    "        new_predictionsA = predictionsA\n",
    "        new_predictionsB = predictionsB\n",
    "        \n",
    "        for i in range(len(new_predictionsA)):\n",
    "            # Check that the 2 predictions are for the same file\n",
    "            file_nameA = new_predictionsA[i][2]\n",
    "            file_nameB = new_predictionsB[i][2]\n",
    "            \n",
    "            if file_nameA != file_nameB:\n",
    "                print file_nameA + \"and \" + file_nameB + \" don't match!\"\n",
    "                return\n",
    "            \n",
    "            # Randomly decide whether to swap the 2 predictions or not\n",
    "            swap = random.randint(0,1)\n",
    "            if swap:\n",
    "                tmp = new_predictionsA[i]\n",
    "                new_predictionsA[i] = new_predictionsB[i]\n",
    "                new_predictionsB[i] = tmp\n",
    "        \n",
    "        # Calculate new mean difference\n",
    "        new_mean_difference = abs(compute_accuracy(new_predictionsA) - compute_accuracy(new_predictionsB))\n",
    "        \n",
    "        if new_mean_difference >= mean_difference:\n",
    "            s += 1.0\n",
    "        \n",
    "        # p-value\n",
    "        return (s + 1.0)/(R + 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the 3 systems using permutation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm_with_unigrams_cross_validate(presence=False)\n",
    "# svm_with_unigrams_cross_validate(presence=True)\n",
    "svm_with_doc2vec_cross_validate()\n",
    "\n",
    "predictions_svm_freqs = predictions_for_svm_unigrams(presence=False)\n",
    "predictions_svm_pres = predictions_for_svm_unigrams(presence=True)\n",
    "predictions_svm_doc2vec = predictions_for_svm_doc2vec()\n",
    "\n",
    "print \"Comparing SVM + frequencies with SVM + doc2vec...\"\n",
    "print \"P-value: \" + str(perm_test(predictions_svm_freqs, predictions_svm_doc2vec, 5000))\n",
    "\n",
    "print \"Comparing SVM + presence with SVM + doc2vec...\"\n",
    "print \"P-value: \" + str(perm_test(predictions_svm_pres, predictions_svm_doc2vec, 5000))\n",
    "\n",
    "print \"Comparing SVM + frequencies with SVM + presence...\"\n",
    "print \"P-value: \" + str(perm_test(predictions_svm_freqs, predictions_svm_pres, 5000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & preprocess AMAZON datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['beauty', 'food', 'music', 'instruments', 'videos', 'games']\n",
    "\n",
    "paths = [\n",
    "    '../beauty.json',\n",
    "    '../food.json',\n",
    "    '../music.json',\n",
    "    '../instruments.json',\n",
    "    '../videos.json',\n",
    "    '../games.json',\n",
    "]\n",
    "\n",
    "beauty_pos = []\n",
    "beauty_neg = []\n",
    "food_pos = []\n",
    "food_neg = []\n",
    "music_pos = []\n",
    "music_neg = []\n",
    "instruments_pos = []\n",
    "instruments_neg = []\n",
    "videos_pos = []\n",
    "videos_neg = []\n",
    "games_pos = []\n",
    "games_neg = []\n",
    "\n",
    "def load_amazon_datasets():\n",
    "    \n",
    "    for i in range(len(paths)):\n",
    "        with open(paths[i]) as f:\n",
    "            json_objs = f.read().split('\\n')\n",
    "            \n",
    "            idx = 1\n",
    "            for json_obj in json_objs:\n",
    "                if json_obj != '':\n",
    "                    data = json.loads(json_obj)\n",
    "                    review_name = categories[i] + str(idx)\n",
    "                    rating = float(data['overall'])\n",
    "                    text = data['reviewText']\n",
    "                    sent = 'NEUTRAL'\n",
    "                    if rating == 5.0:\n",
    "                        sent = 'POS'\n",
    "                    if rating == 1.0:\n",
    "                        sent = 'NEG'\n",
    "                    features = simple_preprocess(text)\n",
    "                    \n",
    "                    if categories[i] == 'beauty':\n",
    "                        if sent == 'POS':\n",
    "                            beauty_pos.append((sent, review_name, features))\n",
    "                        elif sent == 'NEG':\n",
    "                            beauty_neg.append((sent, review_name, features))\n",
    "                    elif categories[i] == 'food':\n",
    "                        if sent == 'POS':\n",
    "                            food_pos.append((sent, review_name, features))\n",
    "                        elif sent == 'NEG':\n",
    "                            food_neg.append((sent, review_name, features))\n",
    "                    elif categories[i] == 'music':\n",
    "                        if sent == 'POS':\n",
    "                            music_pos.append((sent, review_name, features))\n",
    "                        elif sent == 'NEG':\n",
    "                            music_neg.append((sent, review_name, features))\n",
    "                    elif categories[i] == 'instruments':\n",
    "                        if sent == 'POS':\n",
    "                            instruments_pos.append((sent, review_name, features))\n",
    "                        elif sent == 'NEG':\n",
    "                            instruments_neg.append((sent, review_name, features))\n",
    "                    elif categories[i] == 'videos':\n",
    "                        if sent == 'POS':\n",
    "                            videos_pos.append((sent, review_name, features))\n",
    "                        elif sent == 'NEG':\n",
    "                            videos_neg.append((sent, review_name, features))\n",
    "                    elif categories[i] == 'games':\n",
    "                        if sent == 'POS':\n",
    "                            games_pos.append((sent, review_name, features))\n",
    "                        elif sent == 'NEG':\n",
    "                            games_neg.append((sent, review_name, features))\n",
    "                    \n",
    "                    idx += 1\n",
    "\n",
    "def get_balanced_test_sets(pos_set, neg_set):\n",
    "    return random.sample(pos_set, 100) + random.sample(neg_set, 100)\n",
    "\n",
    "load_amazon_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and predict phases on SVM + freq/pres & SVM + doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trained_svm(training_set, presence):\n",
    "    feature_indices = get_feature_indices(training_set)\n",
    "\n",
    "    # Train the SVM model on the training set\n",
    "    formatted_training_set = []\n",
    "\n",
    "    for (sentiment, file_name, features) in training_set:\n",
    "        feature_freqs = {}\n",
    "        for w in features:\n",
    "            if presence or w not in feature_freqs:\n",
    "                feature_freqs[w] = 1\n",
    "            else:\n",
    "                feature_freqs[w] += 1\n",
    "\n",
    "        feature_vec = []\n",
    "        for word, count in feature_freqs.items():\n",
    "            feature_vec.append((feature_indices[word], count))\n",
    "\n",
    "        list.sort(feature_vec, key=lambda x: x[0])\n",
    "\n",
    "        sent_val = 1 if sentiment == \"POS\" else -1\n",
    "        formatted_training_set.append((sent_val, feature_vec))\n",
    "\n",
    "    model = svmlight.learn(formatted_training_set, type='classification')\n",
    "    return model\n",
    "\n",
    "def test_svm(svm_model, training_set, test_set, presence):\n",
    "    feature_indices = get_feature_indices(training_set + test_set)\n",
    "    \n",
    "    formatted_test_set = []\n",
    "    \n",
    "    for (sentiment, file_name, features) in test_set:\n",
    "        feature_freqs = {}\n",
    "        for w in features:\n",
    "            if presence or w not in feature_freqs:\n",
    "                feature_freqs[w] = 1\n",
    "            else:\n",
    "                feature_freqs[w] += 1\n",
    "\n",
    "        feature_vec = []\n",
    "        for word, count in feature_freqs.items():\n",
    "            feature_vec.append((feature_indices[word], count))\n",
    "\n",
    "        list.sort(feature_vec, key=lambda x: x[0])\n",
    "        formatted_test_set.append((0, feature_vec))\n",
    "\n",
    "    predictions = svmlight.classify(svm_model, formatted_test_set)\n",
    "\n",
    "    # Format the predictions into truples of (predicted_sentiment, actual_sentiment, file_name)\n",
    "    formatted_predictions = []\n",
    "    idx = 0\n",
    "    for (sentiment, file_name, features) in test_set:\n",
    "        formatted_predictions.append((\"POS\" if predictions[idx] > 0 else \"NEG\", sentiment, file_name))\n",
    "        idx += 1\n",
    "    \n",
    "    return formatted_predictions\n",
    "\n",
    "def get_trained_svm_d2v(d2v_file, training_set):\n",
    "    doc2vec_model = Doc2Vec.load(d2v_file)\n",
    "        \n",
    "    # Train the SVM model on the training set using Doc2Vec embeddings\n",
    "    formatted_training_set = []\n",
    "\n",
    "    for (sentiment, file_name, features) in training_set:\n",
    "        vec = [(i+1, p) for i, p in enumerate(doc2vec_model.infer_vector(features))]\n",
    "        sent_val = 1 if sentiment == \"POS\" else -1\n",
    "        formatted_training_set.append((sent_val, vec))\n",
    "      \n",
    "    svm_model = svmlight.learn(formatted_training_set, type='classification')\n",
    "    return svm_model, doc2vec_model\n",
    "\n",
    "def test_svm_d2v(d2v_model, svm_model, test_set):\n",
    "    formatted_test_set = []\n",
    "    \n",
    "    for (sentiment, file_name, features) in test_set:\n",
    "        vec = [(i+1, p) for i, p in enumerate(d2v_model.infer_vector(features))]\n",
    "        formatted_test_set.append((0, vec))\n",
    "\n",
    "    predictions = svmlight.classify(svm_model, formatted_test_set)\n",
    "\n",
    "    # Format the predictions into truples of (predicted_sentiment, actual_sentiment, file_name)\n",
    "    formatted_predictions = []\n",
    "    idx = 0\n",
    "    for (sentiment, file_name, features) in test_set:\n",
    "        formatted_predictions.append((\"POS\" if predictions[idx] > 0 else \"NEG\", sentiment, file_name))\n",
    "        idx += 1\n",
    "\n",
    "    return formatted_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_freqs = get_trained_svm(remaining_data_set, presence=False)\n",
    "svm_pres = get_trained_svm(remaining_data_set, presence=True)\n",
    "svm_d2v, d2v_model = get_trained_svm_d2v('./models/model73.modelFile', remaining_data_set)\n",
    "print \"Trained all 3 classifiers!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the 3 systems on 200 balanced reviews of each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_3_systems(data_set_pos, data_set_neg):\n",
    "    acc1 = []\n",
    "    acc2 = []\n",
    "    acc3 = []\n",
    "    freqs_pres = []\n",
    "    freqs_doc = []\n",
    "    pres_doc = []\n",
    "    for _ in range(10):\n",
    "        test_set = get_balanced_test_sets(data_set_pos, data_set_neg)\n",
    "        preds_svm_freqs = test_svm(svm_freqs, remaining_data_set, test_set, presence=False)\n",
    "        preds_svm_pres = test_svm(svm_pres, remaining_data_set, test_set, presence=True)\n",
    "        preds_svm_doc2vec = test_svm_d2v(d2v_model, svm_d2v, test_set)\n",
    "        freqs_pres.append(perm_test(preds_svm_freqs, preds_svm_pres, 5000))\n",
    "        freqs_doc.append(perm_test(preds_svm_freqs, preds_svm_doc2vec, 5000))\n",
    "        pres_doc.append(perm_test(preds_svm_pres, preds_svm_doc2vec, 5000))\n",
    "        a1 = compute_accuracy(preds_svm_freqs)\n",
    "        a2 = compute_accuracy(preds_svm_pres)\n",
    "        a3 = compute_accuracy(preds_svm_doc2vec)\n",
    "        acc1.append(a1)\n",
    "        acc2.append(a2)\n",
    "        acc3.append(a3)\n",
    "    print \"SVM+freqs: \" + str(acc1)\n",
    "    print \"SVM+pres: \" + str(acc2) \n",
    "    print \"SVM+doc: \" + str(acc3)\n",
    "    print \"freqs vs. pres: \" + str(freqs_pres)\n",
    "    print \"freqs vs. doc: \" + str(freqs_doc)\n",
    "    print \"pres vs. doc: \" + str(pres_doc)\n",
    "    print \"\\n\"\n",
    "\n",
    "print \"Beauty:\"\n",
    "test_3_systems(beauty_pos, beauty_neg)\n",
    "\n",
    "print \"Grocery and Gourmet Food:\"\n",
    "test_3_systems(food_pos, food_neg)\n",
    "\n",
    "print \"Digital music:\"\n",
    "test_3_systems(music_pos, music_neg)\n",
    "\n",
    "print \"Musical Instruments:\"\n",
    "test_3_systems(instruments_pos, instruments_neg)\n",
    "\n",
    "print \"Amazon Instant Video:\"\n",
    "test_3_systems(videos_pos, videos_neg)\n",
    "\n",
    "print \"Video games:\"\n",
    "test_3_systems(games_pos, games_neg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
